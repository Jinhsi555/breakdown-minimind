{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744f4bee",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "Tokenizer（分词器）将文本分割成单词或子词并转化为数组编号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a9c0e",
   "metadata": {},
   "source": [
    "## 训练简单分词器\n",
    "### 初始化\n",
    "使用字节对编码（BPE）算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efbbf17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer\n",
    ")\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548918e5",
   "metadata": {},
   "source": [
    "### 定义特殊标记\n",
    "数据集中存在一些不希望被分词的特殊标记，应定义为特殊标记，防止出现错误的分词情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3ba0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=256,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True,\n",
    "    # 使用 ByteLevel 预分词器的初始字母表。\n",
    "    # 作用: 提供一个基础的字符集（例如 ASCII 字符集），确保分词器能够处理所有可能的字符。\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c3346",
   "metadata": {},
   "source": [
    "### 读取数据\n",
    "使用JSON Lines（jsonl）格式存储Tokenizer训练数据，分词器内置的训练函数要求训练数据以迭代器的形式传入，因此先获取一个数据读取的生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6934fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file path: e:\\breakdown-minimind\\script\n",
      "Row 1: <s>近年来，人工智能技术迅速发展，深刻改变了各行各业的面貌。机器学习、自然语言处理、计算机视觉等领域的突破性进展，使得智能产品和服务越来越普及。从智能家居到自动驾驶，再到智能医疗，AI的应用场景正在快速拓展。随着技术的不断进步，未来的人工智能将更加智能、更加贴近人类生活。</s>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            yield data['text']\n",
    "\n",
    "# 打印当前文件路径\n",
    "print(\"Current file path:\", os.getcwd())\n",
    "data_path = '../toydata/tokenizer_data.jsonl'\n",
    "data_iterator = read_texts_from_jsonl(data_path)\n",
    "print(f\"Row 1: {next(data_iterator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c1690",
   "metadata": {},
   "source": [
    "### 开始训练\n",
    "使用分词器内置函数 `tokenizer.train_from_iterator` 来训练分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fc1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(data_iterator, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54579099",
   "metadata": {},
   "source": [
    "### 设置解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3ecfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6ed8",
   "metadata": {},
   "source": [
    "然后检查特殊标记是否正确被处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab98d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.token_to_id(\"<unk>\") == 0\n",
    "assert tokenizer.token_to_id(\"<s>\") == 1\n",
    "assert tokenizer.token_to_id(\"</s>\") == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c3457",
   "metadata": {},
   "source": [
    "### 将训练好的分词器保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2aec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/toy_tokenizer\\\\vocab.json', '../model/toy_tokenizer\\\\merges.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dir = '../model/toy_tokenizer'\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, 'tokenizer.json'))\n",
    "tokenizer.model.save(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fc440",
   "metadata": {},
   "source": [
    "### 创建配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863fa171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training and saving completed successfully.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": False,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<unk>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True,\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"content\": \"<s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True,\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"content\": \"</s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True,\n",
    "        },\n",
    "    },\n",
    "    \"additional_special_tokens\": [],\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 32768,\n",
    "    \"pad_token\": \"<unk>\",\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"chat_template\": \"{{ '<s>' + messages[0]['text'] + '</s>' }}\",\n",
    "}\n",
    "\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training and saving completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0faf950",
   "metadata": {},
   "source": [
    "现在已经训练了一个简单的分词器，并将其进行保存，接下来，我们试着加载它，并使用其帮助我们对文本进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e226b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\envs\\MiniMind\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本：[{'text': '失去的东西就要学者去接受，学着放下。'}]\n",
      "应用聊天模板后的文本：<s>失去的东西就要学者去接受，学着放下。</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model/toy_tokenizer\")\n",
    "message = [{\"text\": \"失去的东西就要学者去接受，学着放下。\"}]\n",
    "new_message = tokenizer.apply_chat_template(conversation=message, tokenize=False)\n",
    "print(f\"原始文本：{message}\")\n",
    "print(f\"应用聊天模板后的文本：{new_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6ebaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器词表大小：259\n"
     ]
    }
   ],
   "source": [
    "print(f\"分词器词表大小：{tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d25e2438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看分词结果：\n",
      "{'input_ids': [1, 164, 100, 112, 164, 239, 122, 166, 251, 229, 163, 119, 253, 167, 101, 126, 164, 111, 112, 167, 102, 226, 164, 258, 102, 167, 225, 230, 164, 239, 122, 165, 239, 101, 164, 240, 248, 174, 123, 237, 164, 258, 102, 166, 254, 225, 165, 245, 125, 163, 119, 236, 162, 225, 227, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(new_message)\n",
    "print(f\"查看分词结果：\\n{model_inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7b69ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对分词结果进行解码：\n",
      "<s>失去的东西就要学者去接受，学着放下。</s>\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(model_inputs['input_ids'], skip_special_tokens=False)\n",
    "print(f\"对分词结果进行解码：\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6166a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiniMind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
