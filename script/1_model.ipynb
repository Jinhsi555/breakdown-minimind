{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9bdfa5",
   "metadata": {},
   "source": [
    "# Model\n",
    "å½“ä»Šä¸»æµå¤§æ¨¡å‹ä»æ¶æ„ä¸Šå¤§è‡´å¯åˆ†ä¸ºç¨ å¯†ï¼ˆDenseï¼‰æ¨¡å‹å’Œæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture of Expert, MoEï¼‰æ¨¡å‹ã€‚\n",
    "\n",
    "ç¨ å¯†æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°åœ¨æ¯æ¬¡è®¡ç®—æ—¶éƒ½ä¼šå‚ä¸è¿ç®—ï¼›æ··åˆä¸“å®¶æ¨¡å‹åˆ™å°†ä¸åŒçš„â€œä¸“å®¶â€æ¨¡å—ç»„åˆï¼Œæ ¹æ®è¾“å…¥é€‰æ‹©åˆé€‚çš„ä¸“å®¶å¤„ç†ï¼Œèƒ½åœ¨ä¿è¯æ•ˆæœçš„åŒæ—¶å‡å°‘è®¡ç®—é‡å’Œå‚æ•°é‡ã€‚\n",
    "\n",
    "MiniMind æ¨¡å‹åœ¨Llama 3.1 çš„åŸºç¡€ä¸Šè®¾è®¡ï¼ŒåŸºäºç»å…¸çš„Transformer Decoder-Only æ¶æ„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5692f5f",
   "metadata": {},
   "source": [
    "## MiniMind Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18ffaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import struct\n",
    "import inspect\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from model.LMConfig import LMConfig\n",
    "from typing import Any, Optional, Tuple, List, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f47e3",
   "metadata": {},
   "source": [
    "### å‡æ–¹æ ¹å±‚å½’ä¸€åŒ–ï¼ˆRMSNormï¼‰\n",
    "RMSNorm æ˜¯å¯¹ LayerNorm çš„æ”¹è¿›ï¼Œç§»é™¤äº†å‡å€¼é¡¹ï¼Œå¯ä»¥è§†ä¸º LayerNormåœ¨å‡å€¼ä¸º 0 æ—¶çš„ç‰¹ä¾‹ã€‚\n",
    "* LayerNorm\n",
    "$$\n",
    "y = \\frac{x- E(x)} {\\sqrt {Var(x)+\\epsilon}} * \\gamma + \\beta\n",
    "$$\n",
    "* RMS Norm\n",
    "$$\n",
    "a_i = \\frac {a_i}{RMS(a)+\\epsilon}*\\gamma, \\ where \\ RMS(a) = \\sqrt { \\frac {1} {n} \\sum_{i=1}^{n} a^2_i}\n",
    "$$\n",
    "RMS Normåœ¨Layer Normçš„åŸºç¡€ä¸Šèˆå¼ƒäº†ä¸­å¿ƒåŒ–æ“ä½œï¼Œä»…ç”¨ç¼©æ”¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œå…¶ä¸æ”¹å˜æ•°æ®åŸæœ¬çš„åˆ†å¸ƒï¼Œæœ‰åˆ©äºæ¿€æ´»å‡½æ•°è¾“å‡ºçš„ç¨³å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac41d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # x: [batch_size, seq_len, dim]\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)  # è¾“å‡ºä¸è¾“å…¥çš„æ•°æ®ç±»å‹ä¸€è‡´ï¼Œé¿å…åç»­è®¡ç®—ç ´åæ··åˆç²¾åº¦è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8d57c",
   "metadata": {},
   "source": [
    "## Rotary Position Embedding(RoPE) æ—‹è½¬ä½ç½®ç¼–ç \n",
    "### Rotary Position Embedding, RoPE\n",
    "\n",
    "æ—‹è½¬ä½ç½®ç¼–ç æ˜¯ä¸€ç§èƒ½å°†ç›¸å¯¹ä½ç½®ä¿¡æ¯é›†æˆåˆ° self-attention ä¸­, è¿›è€Œæå‡ transformer æ¶æ„æ€§èƒ½çš„ä½ç½®ç¼–ç æ–¹å¼, å’Œç»å¯¹ä½ç½®ç¼–ç ç›¸æ¯”, RoPE å…·æœ‰å¾ˆå¥½çš„å¤–æ¨æ€§, æ˜¯ç›®å‰çš„ä¸»æµä½ç½®ç¼–ç æ–¹å¼.\n",
    "\n",
    "å¤–æ¨æ€§çš„è§£é‡Š, é€šä¿—æ¥è¯´å°±æ˜¯è®­ç»ƒçš„æ—¶å€™é™åˆ¶äº† 512 çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé‚£ä¹ˆæ¨ç†æ—¶å¦‚æœé¢å¯¹è¶…è¿‡è¯¥é•¿åº¦çš„æ–‡æœ¬ï¼ŒLLM å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†.\n",
    "\n",
    "- **ç»å¯¹ä½ç½®ç¼–ç **\n",
    "\n",
    "ç»å¯¹ä½ç½®ç¼–ç æ˜¯æ—©æœŸ Transformer æ¶æ„é‡‡ç”¨çš„ç»å¯¹ä½ç½®ç¼–ç æ–¹æ¡ˆï¼ŒåŠé‚£ä¸ªæ¯ä¸ªä½ç½®æ˜ å°„ä¸ºå›ºå®šçš„å‘é‡è¡¨ç¤º.\n",
    "\n",
    "$$f_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i,i)=\\boldsymbol{W}_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i+\\boldsymbol{p}_i)$$\n",
    "\n",
    "å…¶ä¸­ç¼–ç å‘é‡ $p_i$ çš„è®¡ç®—ä½¿ç”¨å¦‚ä¸‹å…¬å¼ï¼š\n",
    "\n",
    "$$\\boldsymbol{p}_{i,2t}=\\sin\\left(k/1000^{2t/d}\\right), \\boldsymbol{p}_{i,2t+1}=\\cos\\left(k/1000^{2t/d}\\right)$$\n",
    "\n",
    "æ­£å¦‚å…¶åï¼Œç»å¯¹ä½ç½®ç¼–ç åªè€ƒè™‘äº†è¾“å…¥åºåˆ—ä¸­çš„ç»å¯¹ä½ç½®å…³ç³»ï¼Œå¯¹äº token ä¹‹é—´çš„ç›¸å¯¹ä¿¡æ¯åˆ™æ²¡æœ‰çº³å…¥è€ƒè™‘.\n",
    "\n",
    "- **æ—‹è½¬ä½ç½®ç¼–ç **\n",
    "\n",
    "å‡å®š query å’Œ key çš„å†…ç§¯æ“ä½œå¯ä»¥è¢«å‡½æ•° g è¡¨ç¤ºï¼Œè¯¥å‡½æ•° g çš„è¾“å…¥æ˜¯è¯åµŒå…¥å‘é‡ $x_m, x_n$ å’Œå®ƒä»¬ä¹‹é—´çš„ç›¸å¯¹ä½ç½® $m-n$:\n",
    "\n",
    "$$<f_q(x_m ,m), f_k(x_n, n)>=g(x_m, x_n, m, n)$$\n",
    "\n",
    "æ—‹è½¬ä½ç½®ç¼–ç å°±æ˜¯æ‰¾åˆ°ä¸€ä¸ªä½¿ä¸Šå¼æˆç«‹çš„ä½ç½®ç¼–ç æ–¹å¼. \n",
    "\n",
    "å‡ºäºè®¤è¯†çš„ç›®çš„ï¼Œæˆ‘ä»¬çœç•¥å¤æ‚çš„æ•°å­¦æ¨å¯¼ï¼Œç›´æ¥çœ‹ RoPE çš„çš„ç»“è®ºï¼š\n",
    "\n",
    "å­˜åœ¨è¿™æ ·ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼š\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^d=\\underbrace{\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\end{pmatrix}}_{\\boldsymbol{W}_m}$$\n",
    "\n",
    "å…¶ä¸­ï¼Œ$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å°† query å’Œ key çš„å†…ç§¯æ“ä½œè½¬æ¢ä¸ºä¸åŸå§‹å‘é‡ $x$ ç›¸å…³çš„ä»¥ä¸‹ç­‰ä»·å½¢å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\boldsymbol{q}_m^\\mathbf{T}\\boldsymbol{k}_n=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)^\\mathbf{T}\\left(\\boldsymbol{R}_{\\Theta,n}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\\right)=\\boldsymbol{x}_m^\\mathbf{T}\\boldsymbol{W}_q\\boldsymbol{R}_{\\Theta,n-m}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼Œ $\\boldsymbol{R}_{\\Theta,n-m}^d=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\right)^\\mathbf{T}\\boldsymbol{R}_{\\Theta,n}^d$.\n",
    "\n",
    "ç”±äº $\\boldsymbol{R}_{\\Theta,m}^d$ çš„ç¨€ç–æ€§ï¼Œç›´æ¥ä½¿ç”¨çŸ©é˜µä¹˜æ³•ä¼šæµªè´¹ç®—åŠ›ï¼Œå› æ­¤ä»£ç ä¸­é‡‡ç”¨ä¸‹è¿°æ–¹å¼å®ç°ï¼š\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^{d}\\boldsymbol{x}=\\begin{pmatrix}x_{0}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\\\\\vdots\\\\x_{d-2}\\\\x_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_{0}\\\\\\cos m\\theta_{0}\\\\\\cos m\\theta_{1}\\\\\\cos m\\theta_{1}\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}+\\begin{pmatrix}-x_{1}\\\\x_{0}\\\\-x_{3}\\\\x_{2}\\\\\\vdots\\\\-x_{d-1}\\\\x_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_{0}\\\\\\sin m\\theta_{0}\\\\\\sin m\\theta_{1}\\\\\\sin m\\theta_{1}\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ecd80b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.7080e-01, -1.6984e+00,  1.6788e-01,  6.8527e-01],\n",
       "          [-1.9820e+00,  1.1438e+00, -4.4433e-01, -2.2123e+00],\n",
       "          [-1.5191e-01, -4.3824e-01, -2.0113e-01,  8.7780e-01],\n",
       "          [-6.7896e-01,  5.6246e-01,  1.2106e+00,  5.2982e-01]],\n",
       "\n",
       "         [[-1.4986e-01, -7.7348e-01,  1.9079e+00, -1.2188e+00],\n",
       "          [ 1.6841e+00, -6.7206e-01,  4.2487e-01, -1.4881e-01],\n",
       "          [ 1.1451e-01, -1.2953e+00, -5.4956e-01,  1.6159e-01],\n",
       "          [ 1.2667e+00, -4.9180e-01,  2.1124e-04,  1.5214e+00]],\n",
       "\n",
       "         [[-1.3046e+00, -7.0576e-01,  1.6761e+00,  1.5808e+00],\n",
       "          [-1.3511e+00, -1.7136e+00,  5.1442e-01,  1.6953e+00],\n",
       "          [-9.2680e-01, -5.2265e-01, -2.7381e-02,  9.0223e-01],\n",
       "          [ 6.3457e-01, -1.5455e+00, -5.9622e-02,  4.1881e-01]],\n",
       "\n",
       "         [[ 5.9142e-01, -5.2605e-01,  1.0153e+00, -8.7640e-01],\n",
       "          [-5.6186e-01,  3.0564e-01, -9.4060e-01,  6.1876e-01],\n",
       "          [-2.0351e-01,  1.4860e+00,  9.7095e-01, -1.0856e-01],\n",
       "          [ 9.2156e-01, -1.4080e+00,  6.1639e-01,  1.1279e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7353e-01,  7.1898e-01,  1.7212e+00,  2.7533e-01],\n",
       "          [-1.2796e+00,  7.5824e-01,  8.1042e-01, -9.0063e-01],\n",
       "          [ 1.2770e+00, -9.1721e-02, -1.0585e+00,  5.0465e-01],\n",
       "          [ 6.8998e-01, -1.1123e+00, -1.6040e+00, -3.3937e-01]],\n",
       "\n",
       "         [[ 1.1913e-02,  3.9630e-01, -5.5827e-02, -1.6305e+00],\n",
       "          [-1.1148e+00,  1.0506e+00, -9.1706e-01, -7.6611e-03],\n",
       "          [ 3.3618e-01, -2.7960e-01,  7.6363e-01, -1.3212e+00],\n",
       "          [ 1.3832e-01,  6.6700e-01,  1.8743e+00, -8.9682e-01]],\n",
       "\n",
       "         [[ 7.9927e-01,  9.7743e-01, -2.2127e-01, -6.5019e-01],\n",
       "          [-1.7178e+00,  1.2308e+00,  1.2274e+00,  8.6013e-02],\n",
       "          [ 4.9047e-01,  1.3192e+00,  3.5724e-01,  3.5257e-01],\n",
       "          [ 2.8495e-01, -1.1329e+00,  9.6510e-01, -3.3133e-01]],\n",
       "\n",
       "         [[ 1.1609e+00,  7.7206e-01,  6.6085e-01, -5.2660e-01],\n",
       "          [ 9.1949e-01,  1.9512e+00, -7.1259e-01, -1.1274e+00],\n",
       "          [-6.1609e-01,  1.5103e+00, -2.5987e-02,  3.1695e+00],\n",
       "          [-2.9439e-01,  9.9413e-01,  7.3966e-01, -3.4241e-01]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åœ¨ RoPE ä¸­é¢„å…ˆè®¡ç®—æ—‹è½¬è§’åº¦å¯¹åº”çš„å¤æ•°ï¼ˆcosÎ¸ + iÂ·sinÎ¸ï¼‰å€¼ mÎ¸\n",
    "def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))  # \\theta_i = 10000^{-2i/d}, i \\in [0, 1, ..., d/2-1]\n",
    "    t = torch.arange(0, end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # [seq_len, dim]\n",
    "    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)\n",
    "    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    def rotate_half(x):\n",
    "        # å°† x çš„å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†ï¼ˆå–åï¼‰è¿›è¡Œäº¤æ¢ï¼Œä»£æ›¿ sin çš„å–å\n",
    "        return torch.cat([-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]], dim=-1)\n",
    "    \n",
    "    # q, k: [batch_size, seq_len, num_heads, head_dim]\n",
    "    # å¯¹ q, k å’Œ cos, sin è¿›è¡Œå¹¿æ’­è¿ç®—ï¼Œéœ€è¦å…ˆåŒ¹é…ç»´åº¦\n",
    "    # cos, sin [seq_len, head_dim] -> [(1), seq_len, 1, head_dim] å³å¯¹æ‰€æœ‰ batch, head è¿›è¡Œç›¸åŒçš„å¹¿æ’­è¿ç®—\n",
    "    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))\n",
    "    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))\n",
    "    return q_embed, k_embed\n",
    "\n",
    "xq,  xk = torch.randn((2,  4,  4,  4)), torch.randn((2,  4,  4,  4)) # (batch_size,  sequence_length,  num_heads,  head_dim)\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(4,  4)\n",
    "q_embed, k_embed = apply_rotary_pos_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "q_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd577dd",
   "metadata": {},
   "source": [
    "### Attention\n",
    "æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer æ¶æ„çš„æ ¸å¿ƒç»„ä»¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é•¿åºåˆ—å†…å„å…ƒç´ é—´çš„ä¾èµ–å…³ç³»ï¼Œé€šè¿‡è®¡ç®—è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®å…ƒç´ é—´çš„æ³¨æ„åŠ›å¾—åˆ†ï¼Œå¯¹é‡è¦æ€§è¿›è¡Œå»ºæ¨¡\n",
    "\n",
    "åœ¨ MiniMindLM æ¨¡å‹ä¸­ï¼ŒAttention Block åŒ…å«ä¸‹é¢çš„æœºåˆ¶å’Œæ¨¡å—ï¼š\n",
    "1. GQA (Group Query Attention) åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›\n",
    "2. KV Cache\n",
    "3. SwiGLU\n",
    "#### GQA\n",
    "GQA æ˜¯å¯¹å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ‰©å±•ï¼Œé€šè¿‡å¯¹æŸ¥è¯¢å¤´åˆ†ç»„ï¼Œæé«˜è®¡ç®—æ•ˆç‡\n",
    "\n",
    "GQA å°† h ä¸ªæŸ¥è¯¢å¤´åˆ†ä¸º G ç»„ï¼Œæ¯ç»„åŒ…å« h / G ä¸ªæŸ¥è¯¢å¤´ï¼Œå…±äº«ä¸€ä¸ªå…¬å…±çš„é”®å’Œå€¼\n",
    "\n",
    "**GQA ç›¸æ¯”ä¼ ç»Ÿ MHAï¼Œå‡å°‘äº†é”®å’Œå€¼çš„æ•°é‡ï¼Œé™ä½äº†è®¡ç®—é‡å’Œå†…å­˜å¼€é”€ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦**\n",
    "\n",
    "### KV Cache\n",
    "åœ¨è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ä¸­ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–°çš„ tokenï¼Œæ¨¡å‹éƒ½éœ€è¦è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œä»¥ç¡®å®šå½“å‰ä½ç½®ä¸ä¹‹å‰æ‰€æœ‰ä½ç½®çš„ç›¸å…³æ€§.\n",
    "\n",
    "æ¯”å¦‚ä»¥ä¸‹å†…å®¹ï¼š\n",
    "\n",
    "1. seq = [tok1] (ä½ç½® 1):\n",
    "\n",
    "   S_1 = [ (Q1 * K1^T) / sqrt(d_k) ] (åªæœ‰è‡ªå·±å’Œè‡ªå·±çš„åˆ†æ•°)\n",
    "\n",
    "   A_1 = softmax(S_1) = [1.0] (å”¯ä¸€é€‰é¡¹ï¼Œæƒé‡ä¸º1)\n",
    "\n",
    "   Output_1 = 1.0 * V1 = V1\n",
    "\n",
    "2. seq = [tok1, tok2] (è®¡ç®—ä½ç½® 2 çš„è¾“å‡º):\n",
    "\n",
    "   S_2 = [ (Q2 * K1^T) / sqrt(d_k), (Q2 * K2^T) / sqrt(d_k) ] (ä½ç½®2å¯¹ä½ç½®1å’Œä½ç½®2çš„åˆ†æ•°)\n",
    "\n",
    "   A_2 = softmax(S_2) = [a_21, a_22] (å¯¹è¿™ä¸¤ä¸ªåˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œa_21 + a_22 = 1)\n",
    "\n",
    "   Output_2 = a_21 * V1 + a_22 * V2\n",
    "\n",
    "3. seq = [tok1, tok2, tok3] (è®¡ç®—ä½ç½® 3 çš„è¾“å‡º):\n",
    "\n",
    "   S_3 = [ (Q3 * K1^T) / sqrt(d_k), (Q3 * K2^T) / sqrt(d_k), (Q3 * K3^T) / sqrt(d_k) ] (ä½ç½®3å¯¹ä½ç½®1,2,3çš„åˆ†æ•°)\n",
    "\n",
    "   A_3 = softmax(S_3) = [a_31, a_32, a_33] (å¯¹è¿™ä¸‰ä¸ªåˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œa_31 + a_32 + a_33 = 1)\n",
    "   \n",
    "   Output_3 = a_31 * V1 + a_32 * V2 + a_33 * V3\n",
    "\n",
    "ä¸éš¾å‘ç°ï¼Œå¤§æ¨¡å‹ç”Ÿæˆä¸€ä¸ª token åçš„æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œæ€»ä¼šç”¨åˆ° token åºåˆ—çš„å†å² KV å€¼ï¼Œå¯¼è‡´é‡å¤è®¡ç®—ï¼ŒKV Cache çš„è®¾è®¡æ­£æ˜¯ä¸ºäº†é€šè¿‡ç¼“å­˜å†å² KV å€¼ï¼ŒèŠ‚çœè®¡ç®—å¼€é”€.\n",
    "\n",
    "KV Cache èƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©å¤§æ¨¡å‹æ¨ç†æ—¶çš„æ˜¾å­˜å ç”¨.\n",
    "\n",
    "æ³¨æ„åŠ›æœºåˆ¶æ˜¯åœ¨**è®¡ç®—æŸä¸ªä½ç½®çš„è¾“å‡ºæ—¶ï¼Œå¯¹è¯¥ä½ç½®ä¸æ‰€æœ‰å¯è§ä½ç½®ï¼ˆå·²ç”Ÿæˆçš„ä½ç½®ï¼‰çš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œä¸€æ¬¡æ€§çš„ `softmax` å½’ä¸€åŒ–**ã€‚\n",
    "\n",
    "#### ğŸ“Œ æ€»ç»“ä¸å…³é”®ç‚¹\n",
    "\n",
    "1.  **`softmax` per Row (per Query)ï¼š** `softmax` æ“ä½œæ˜¯é’ˆå¯¹**ä¸€ä¸ªç‰¹å®šæŸ¥è¯¢ä½ç½® `i`** çš„æ‰€æœ‰ï¼ˆæœªè¢«æ©ç çš„ï¼‰é”®ä½ç½®çš„åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ã€‚å®ƒå‘ç”Ÿåœ¨è®¡ç®—è¯¥æŸ¥è¯¢ä½ç½® `i` çš„è¾“å‡ºå‘é‡ä¹‹å‰ã€‚\n",
    "2.  **æ³¨æ„åŠ›æƒé‡çš„æ„ä¹‰ï¼š** å½’ä¸€åŒ–åçš„æ³¨æ„åŠ›æƒé‡ `a_ij` ä»£è¡¨äº† **ä½ç½® `i`ï¼ˆæŸ¥è¯¢ï¼‰å¯¹ä½ç½® `j`ï¼ˆé”®å€¼ï¼‰çš„â€œå…³æ³¨ç¨‹åº¦â€**ã€‚æ‰€æœ‰æƒé‡ä¹‹å’Œä¸º 1ã€‚\n",
    "3.  **è¾“å‡ºæ˜¯åŠ æƒå’Œï¼š** ä½ç½® `i` çš„è¾“å‡ºæ˜¯å…¶å¯¹æ‰€æœ‰å¯è§ä½ç½® `j` çš„å€¼å‘é‡ `V_j` çš„åŠ æƒå’Œï¼Œæƒé‡å°±æ˜¯ `a_ij`ã€‚\n",
    "4.  **è‡ªå›å½’ç”Ÿæˆä¸­çš„ç¼“å­˜ (KV Cache)ï¼š** åœ¨åƒGPTè¿™æ ·çš„Decoder-onlyæ¨¡å‹è¿›è¡Œè‡ªå›å½’ç”Ÿæˆæ—¶ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰ï¼š\n",
    "    *   å½“ç”Ÿæˆç¬¬ `i` ä¸ª token æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—**å½“å‰**çš„ `Q_i`ã€‚\n",
    "    *   æ‰€æœ‰ä¹‹å‰ä½ç½® `j < i` çš„ `K_j` å’Œ `V_j` å·²ç»ä»ä¹‹å‰çš„æ­¥éª¤ä¸­è®¡ç®—å¹¶**ç¼“å­˜**å¥½äº† (è¿™å°±æ˜¯è‘—åçš„ **KV Cache**)ã€‚\n",
    "    *   å› æ­¤ï¼Œè®¡ç®— `Output_i` åªéœ€è¦ï¼š\n",
    "        *   è®¡ç®— `Q_i`ï¼›\n",
    "        *   ç”¨ `Q_i` å’Œç¼“å­˜çš„ `K_{1:i}` è®¡ç®—åˆ†æ•° `S_i`ï¼›\n",
    "        *   å¯¹ `S_i` åš `softmax` å¾—åˆ° `A_i`ï¼›\n",
    "        *   ç”¨ `A_i` å’Œç¼“å­˜çš„ `V_{1:i}` è®¡ç®—åŠ æƒå’Œ `Output_i`ã€‚\n",
    "    *   è®¡ç®—å®Œ `Output_i` åï¼Œæˆ‘ä»¬ä¼šè®¡ç®—å¹¶ç¼“å­˜**å½“å‰**ä½ç½®çš„ `K_i` å’Œ `V_i`ï¼Œä¾›åç»­ç”Ÿæˆæ­¥éª¤ä½¿ç”¨ã€‚\n",
    "  \n",
    "### SwiGLU\n",
    "SwiGLU æ˜¯ä¸€ç§æ¿€æ´»å‡½æ•°å˜ä½“:\n",
    "$$\n",
    "SwiGLU(x, W, V, b, c) = Swish(xW+b) \\otimes (xV+c)\n",
    "$$\n",
    "å…¶ä¸­ $Swish(x)=x \\cdot \\sigma (\\beta x)$\n",
    "\n",
    "ä¸ä¼ ç»Ÿçš„ ReLU æ¿€æ´»å‡½æ•°ç›¸æ¯”ï¼ŒSwiGLU å…·æœ‰æ›´å¥½çš„å¹³æ»‘æ€§å’Œéçº¿æ€§è¡¨è¾¾èƒ½åŠ›ï¼Œç”±äºå…¶é—¨æ§æœºåˆ¶ï¼Œåœ¨å¤„ç†ä¿¡æ¯ç­›é€‰å’ŒæµåŠ¨æ–¹é¢æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7636a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class MiniMindConfig(PretrainedConfig):\n",
    "    model_type = \"minimind\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout: float = 0.0,\n",
    "        bos_token_id: int = 1,\n",
    "        eos_token_id: int = 2,\n",
    "        hidden_act: str = 'silu',\n",
    "        hidden_size: int = 512,\n",
    "        intermediate_size: int = None,\n",
    "        max_position_embeddings: int = 32768,\n",
    "        num_attention_heads: int = 8,\n",
    "        num_hidden_layers: int = 8,\n",
    "        num_key_value_heads: int = 2,\n",
    "        vocab_size: int = 6400,\n",
    "        rms_norm_eps: float = 1e-5,\n",
    "        rope_theta: int = 1000000,\n",
    "        flash_attn: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.hidden_act = hidden_act\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.flash_attn = flash_attn\n",
    "        \n",
    "        \n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"ä½¿ KV å¤´æ•°é€‚åº” Query å¤´æ•°ï¼Œ æ‰§è¡ŒçŸ©é˜µä¹˜æ³•å¹¶è¡Œè¿ç®—\n",
    "    ç­‰ä»·äº torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    batch_size, seq_len, num_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]  # ç­‰ä»·äº x.unsqueeze(3)\n",
    "        .expand(batch_size, seq_len, num_kv_heads, n_rep, head_dim)\n",
    "        .reshape(batch_size, seq_len, num_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads\n",
    "        assert args.num_attention_heads % self.num_key_value_heads == 0\n",
    "        self.n_local_heads = args.num_attention_heads\n",
    "        self.n_local_kv_heads = args.num_key_value_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.hidden_size // args.num_attention_heads  # query å¤´æ˜ å°„çš„ head_dim\n",
    "        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(args.hidden_size, args.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(args.hidden_size, args.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        self.dropout = args.dropout\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn\n",
    "        \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # æ¥æ”¶ cos å’Œ sin\n",
    "                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "                use_cache=False,\n",
    "                attention_mask: Optional[torch.Tensor] = None,):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        ############## Forward QKV & RoPE ##############\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        \n",
    "        cos, sin = position_embeddings\n",
    "        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])  # æˆªæ–­è‡³ seq_len\n",
    "        \n",
    "        # kv_cache å®ç°\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0], xk], dim=1)  # ç¼“å­˜æ¯ä¸€ä¸ª token çš„ k, v\n",
    "            xv = torch.cat([past_key_value[1], xk], dim=1)\n",
    "        past_kv = (xk, xv) if use_cache else None\n",
    "        \n",
    "        # [batch_size, seq_len, num_heads, head_dim] -> [bsz, num_heads, seq_len, head_dim]\n",
    "        xq, xk, xv = (\n",
    "            xq.transpose(1, 2),\n",
    "            repeat_kv(xk, self.n_rep).transpose(1, 2),\n",
    "            repeat_kv(xv, self.n_rep).transpose(1, 2),\n",
    "        )\n",
    "        \n",
    "        ############ Scaled Dot Production #############\n",
    "        if self.flash and seq_len != 1:\n",
    "            dropout_p = self.dropout if self.training else 0.0\n",
    "            attn_mask = None  # è¿™é‡Œçš„ attention_mask æŒ‡çš„æ˜¯ padding çš„æ©ç \n",
    "            if attention_mask is not None:\n",
    "                attn_mask = attention_mask.view(batch_size, 1, 1, -1).expand(batch_size, self.n_local_heads, seq_len, -1)  # attention_mask å½¢çŠ¶ä¸º [bsz, seq_len] æ‰©å±•åå½¢çŠ¶ä¸º [bsz, n_heads, seq_len, seq_len]\n",
    "                attn_mask = attn_mask.bool()\n",
    "            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=True)\n",
    "        else:\n",
    "            # æ™®é€šæ³¨æ„åŠ›æœºåˆ¶\n",
    "            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # ç¼©æ”¾ç‚¹ç§¯\n",
    "            scores = scores + torch.triu(\n",
    "                torch.full((1, 1, seq_len, seq_len), float(\"inf\"), device=scores.device),\n",
    "                diagonal=1\n",
    "            )\n",
    "            \n",
    "            # å¤„ç† padding çš„æ©ç \n",
    "            if attention_mask is not None:\n",
    "                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9  # padding çš„éƒ¨åˆ†å˜ä¸º -inf\n",
    "                scores += extended_attention_mask\n",
    "                \n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = scores @ xv  # [..., seq_len, seq_len] @ [..., seq_len, head_dim] -> [..., seq_len, head_dim]\n",
    "        \n",
    "        output = output.transpose(1, 2).reshape(batch_size, seq_len, -1)  # -> [batch_size, seq_len, dim] ç­‰ä»·äºå°†æ‰€æœ‰å¤´çš„è¾“å‡ºç»´åº¦æ‹¼æ¥\n",
    "        output = self.resid_dropout(self.o_proj(output))\n",
    "        return output, past_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3081c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å¼ é‡ x ï¼šsize = torch.Size([2, 4, 512])ï¼ŒRoPE æ—‹è½¬è§’ï¼š size = torch.Size([4, 64])\n",
      "è¾“å‡º output: size = torch.Size([2, 4, 512]),  kv_cache åŸºæœ¬ä¿¡æ¯ï¼šsize_key = torch.Size([2, 4, 2, 64]), size_value = torch.Size([2, 4, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "minimind_config = MiniMindConfig()\n",
    "attn = Attention(minimind_config)\n",
    "x = torch.randn((2, 4, 512))\n",
    "cos, sin = precompute_freqs_cis(64, 4)\n",
    "output, past_kv = attn(x, (cos, sin), use_cache=True)\n",
    "print(f'è¾“å…¥å¼ é‡ x ï¼šsize = {x.shape}ï¼ŒRoPE æ—‹è½¬è§’ï¼š size = {cos.shape}')\n",
    "print(f'è¾“å‡º output: size = {output.shape},  kv_cache åŸºæœ¬ä¿¡æ¯ï¼šsize_key = {past_kv[0].shape}, size_value = {past_kv[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "584cbe53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0971, -0.0796,  0.0141,  ...,  0.1047, -0.4975, -0.4599],\n",
       "         [ 0.1240, -0.0765, -0.0130,  ...,  0.0044, -0.3687, -0.5107],\n",
       "         [ 0.2767,  0.1954, -0.0747,  ...,  0.0796, -0.2997, -0.4138],\n",
       "         [ 0.2565,  0.1989,  0.1925,  ...,  0.1465, -0.1451, -0.1762]],\n",
       "\n",
       "        [[ 0.2987,  0.2173,  0.0833,  ..., -0.1869,  0.2313,  0.1192],\n",
       "         [ 0.0717,  0.3301, -0.0474,  ..., -0.2508,  0.0669,  0.1901],\n",
       "         [ 0.0103,  0.1575, -0.1288,  ..., -0.2612,  0.0579,  0.4255],\n",
       "         [ 0.2397,  0.1856, -0.1839,  ..., -0.2195,  0.0035,  0.2312]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acebbe53",
   "metadata": {},
   "source": [
    "### FeedForward Network\n",
    "å‰é¦ˆå±‚æ¥æ”¶æ¥è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºï¼Œå¯¹è¾“å‡ºæ‰§è¡Œè¿›ä¸€æ­¥çš„çº¿æ€§å˜æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cf789a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "class FeedForward(nn.Module):\n",
    "    # input -> RMSNorm ->       up_proj     -> down_proj -> dropout -> output\n",
    "    #                 \\                       /\n",
    "    #                  -> gate_proj -> SiLU ->\n",
    "    def __init__(self, config: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        if config.intermediate_size is None:\n",
    "            intermediate_size = int(config.hidden_size * 8 / 3)\n",
    "            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)  # å‘ä¸Šå–æ•´åˆ° 64 çš„å€æ•°\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [batch_size, seq_len, hidden_size]\n",
    "        up_out = self.up_proj(x)\n",
    "        gate_out = self.gate_proj(x)\n",
    "        down_out = self.down_proj(self.act_fn(gate_out) * up_out)\n",
    "        return self.dropout(down_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58368ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å¼ é‡ x ï¼šsize = torch.Size([2, 4, 512])\n",
      "è¾“å‡º output: size = torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(minimind_config)\n",
    "x = torch.randn((2, 4, 512))\n",
    "output = ffn(x)\n",
    "print(f'è¾“å…¥å¼ é‡ x ï¼šsize = {x.shape}')\n",
    "print(f'è¾“å‡º output: size = {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a6f0a",
   "metadata": {},
   "source": [
    "### MiniMind Block\n",
    "åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå·²ç»å®Œæˆäº† Attention Layer å’Œ FeedForward Layerï¼Œæ‰€æœ‰å¿…é¡»çš„ç»„ä»¶éƒ½å·²ç»å…·å¤‡ï¼Œå¼€å§‹æ„å»º MiniMind Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1ef8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniMindBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, config: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_dim = self.hidden_size // self.num_attention_heads\n",
    "        self.self_attn = Attention(config)\n",
    "        \n",
    "        self.layer_id = layer_id\n",
    "        self.input_layernorm = RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = FeedForward(config)\n",
    "        \n",
    "    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):\n",
    "        residual = hidden_states\n",
    "        hidden_states, present_key_value = self.self_attn(\n",
    "            self.input_layernorm(hidden_states),\n",
    "            position_embeddings,\n",
    "            past_key_value=past_key_value,\n",
    "            use_cache=use_cache,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        hidden_states = residual + hidden_states  # Multihead Self Attention æ®‹å·®è¿æ¥\n",
    "        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))  # Feed Forward æ®‹å·®è¿æ¥\n",
    "        return hidden_states, present_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24195b37",
   "metadata": {},
   "source": [
    "### MiniMindLM(Dense)\n",
    "ä»¥ MiniMind Block ä¸ºåŸºæœ¬ç»„ä»¶ï¼Œå¯¹ MiniMindLM è¿›è¡Œæœ€åçš„ç»„è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4455717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "class MiniMindModel(PreTrainedModel):\n",
    "    def __init__(self, config: MiniMindConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layers = nn.ModuleList([MiniMindBlock(i, config) for i in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        \n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(\n",
    "            config.hidden_size // config.num_attention_heads,\n",
    "            end=config.max_position_embeddings,\n",
    "            theta=config.rope_theta\n",
    "        )\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "        \n",
    "    def forward(self,\n",
    "                input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,\n",
    "                use_cache: bool = False,\n",
    "                **kwargs):\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        past_key_values = past_key_values or [None] * len(self.layers)\n",
    "        # ç¡®å®šå½“å‰åœ¨åºåˆ—ä¸­çš„èµ·å§‹ä½ç½®, å¤„ç†å¢é‡ç”Ÿæˆ\n",
    "        # past_key_values[0]ï¼šæ¨¡å‹ç¬¬ä¸€å±‚çš„ç¼“å­˜ (K_cache, V_cache)\n",
    "        # past_key_values[0][0].shape[1] è·å– K_cache çš„åºåˆ—é•¿åº¦\n",
    "        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0\n",
    "        \n",
    "        hidden_state = self.dropout(self.embed_tokens(input_ids))\n",
    "        \n",
    "        position_embeddings = (\n",
    "            self.freqs_cos[start_pos:start_pos + seq_length],\n",
    "            self.freqs_sin[start_pos:start_pos + seq_length]\n",
    "        )\n",
    "        \n",
    "        presents = []\n",
    "        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):\n",
    "            hidden_state, present = layer(\n",
    "                hidden_state,\n",
    "                position_embeddings,\n",
    "                past_key_value=past_key_value,\n",
    "                use_cache=use_cache,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            presents.append(present)  # æ‰€æœ‰ MultiHead Attention å±‚çš„ kv cache\n",
    "            \n",
    "        hidden_state = self.norm(hidden_state)\n",
    "        return hidden_state, presents\n",
    "    \n",
    "class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):  # è‡ªå›å½’ç”Ÿæˆå‡½æ•°ç±»\n",
    "    config_class = MiniMindConfig\n",
    "    \n",
    "    def __init__(self, config: MiniMindConfig = None):\n",
    "        self.config = config or MiniMindConfig()\n",
    "        super().__init__(self.config)\n",
    "        self.model = MiniMindModel(self.config)\n",
    "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
    "        self.model.embed_tokens.weight = self.lm_head.weight  # æƒé‡ç»‘å®šï¼Œä¿è¯è¯å‘é‡åµŒå…¥å’Œè¾“å‡ºå¤´çš„äº’é€†æ€§\n",
    "        self.OUT = CausalLMOutputWithPast()\n",
    "        \n",
    "    def forward(self, \n",
    "                input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,\n",
    "                use_cache: bool = False,\n",
    "                logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "                **args):\n",
    "        hidden_state, past_kvs = self.model(\n",
    "            input_ids, \n",
    "            attention_mask, \n",
    "            past_key_values, \n",
    "            use_cache,\n",
    "            **args)\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        logits = self.lm_head(hidden_state[:, slice_indices, :])  # [batch_size, seq_len, hidden_size] è‡ªå›å½’åªéœ€è¦å¯¹ hidden_state æœ€æ–°ç”Ÿæˆçš„ token è¿›è¡Œè®¡ç®—å³å¯\n",
    "        self.OUT.__setitem__('last_hidden_state', hidden_state)\n",
    "        self.OUT.__setitem__('logits', logits)\n",
    "        self.OUT.__setitem__('past_key_values', past_kvs)\n",
    "        return self.OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f93733b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4322, -0.3280,  1.5830,  ...,  0.7104, -0.2311,  0.4662],\n",
       "         [ 0.3969, -0.3302,  1.2280,  ...,  0.2606, -0.3116,  0.1977],\n",
       "         [ 0.8346, -0.1078,  0.9870,  ...,  0.4537, -0.2502, -0.0225],\n",
       "         [ 0.9640, -0.0186,  1.0079,  ...,  0.4181, -0.0878, -0.0524]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MiniMind_Dense = MiniMindForCausalLM()\n",
    "input_ids = torch.Tensor([1, 3, 5, 7]).long().reshape(1, 4)\n",
    "OUT = MiniMind_Dense(input_ids, use_cache=True)\n",
    "OUT.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec4890",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiniMind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
