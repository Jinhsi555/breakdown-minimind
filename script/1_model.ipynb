{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9bdfa5",
   "metadata": {},
   "source": [
    "# Model\n",
    "当今主流大模型从架构上大致可分为稠密（Dense）模型和混合专家模型（Mixture of Expert, MoE）模型。\n",
    "\n",
    "稠密模型中所有参数在每次计算时都会参与运算；混合专家模型则将不同的“专家”模块组合，根据输入选择合适的专家处理，能在保证效果的同时减少计算量和参数量。\n",
    "\n",
    "MiniMind 模型在Llama 3.1 的基础上设计，基于经典的Transformer Decoder-Only 架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5692f5f",
   "metadata": {},
   "source": [
    "## MiniMind Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ffaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import struct\n",
    "import inspect\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from model.LMConfig import LMConfig\n",
    "from typing import Any, Optional, Tuple, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f47e3",
   "metadata": {},
   "source": [
    "### 均方根层归一化（RMSNorm）\n",
    "RMSNorm 是对 LayerNorm 的改进，移除了均值项，可以视为 LayerNorm在均值为 0 时的特例。\n",
    "* LayerNorm\n",
    "$$\n",
    "y = \\frac{x- E(x)} {\\sqrt {Var(x)+\\epsilon}} * \\gamma + \\beta\n",
    "$$\n",
    "* RMS Norm\n",
    "$$\n",
    "a_i = \\frac {a_i}{RMS(a)+\\epsilon}*\\gamma, \\ where \\ RMS(a) = \\sqrt { \\frac {1} {n} \\sum_{i=1}^{n} a^2_i}\n",
    "$$\n",
    "RMS Norm在Layer Norm的基础上舍弃了中心化操作，仅用缩放进行归一化，其不改变数据原本的分布，有利于激活函数输出的稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac41d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # x: [batch_size, seq_len, dim]\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)  # 输出与输入的数据类型一致，避免后续计算破坏混合精度训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8d57c",
   "metadata": {},
   "source": [
    "## Rotary Position Embedding(RoPE) 旋转位置编码\n",
    "### Rotary Position Embedding, RoPE\n",
    "\n",
    "旋转位置编码是一种能将相对位置信息集成到 self-attention 中, 进而提升 transformer 架构性能的位置编码方式, 和绝对位置编码相比, RoPE 具有很好的外推性, 是目前的主流位置编码方式.\n",
    "\n",
    "外推性的解释, 通俗来说就是训练的时候限制了 512 的上下文长度，那么推理时如果面对超过该长度的文本，LLM 可能无法正确处理.\n",
    "\n",
    "- **绝对位置编码**\n",
    "\n",
    "绝对位置编码是早期 Transformer 架构采用的绝对位置编码方案，及那个每个位置映射为固定的向量表示.\n",
    "\n",
    "$$f_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i,i)=\\boldsymbol{W}_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i+\\boldsymbol{p}_i)$$\n",
    "\n",
    "其中编码向量 $p_i$ 的计算使用如下公式：\n",
    "\n",
    "$$\\boldsymbol{p}_{i,2t}=\\sin\\left(k/1000^{2t/d}\\right), \\boldsymbol{p}_{i,2t+1}=\\cos\\left(k/1000^{2t/d}\\right)$$\n",
    "\n",
    "正如其名，绝对位置编码只考虑了输入序列中的绝对位置关系，对于 token 之间的相对信息则没有纳入考虑.\n",
    "\n",
    "- **旋转位置编码**\n",
    "\n",
    "假定 query 和 key 的内积操作可以被函数 g 表示，该函数 g 的输入是词嵌入向量 $x_m, x_n$ 和它们之间的相对位置 $m-n$:\n",
    "\n",
    "$$<f_q(x_m ,m), f_k(x_n, n)>=g(x_m, x_n, m, n)$$\n",
    "\n",
    "旋转位置编码就是找到一个使上式成立的位置编码方式. \n",
    "\n",
    "出于认识的目的，我们省略复杂的数学推导，直接看 RoPE 的的结论：\n",
    "\n",
    "存在这样一个正交矩阵：\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^d=\\underbrace{\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\end{pmatrix}}_{\\boldsymbol{W}_m}$$\n",
    "\n",
    "其中，$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$\n",
    "\n",
    "我们可以将 query 和 key 的内积操作转换为与原始向量 $x$ 相关的以下等价形式：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{q}_m^\\mathbf{T}\\boldsymbol{k}_n=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)^\\mathbf{T}\\left(\\boldsymbol{R}_{\\Theta,n}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\\right)=\\boldsymbol{x}_m^\\mathbf{T}\\boldsymbol{W}_q\\boldsymbol{R}_{\\Theta,n-m}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\n",
    "$$\n",
    "\n",
    "其中， $\\boldsymbol{R}_{\\Theta,n-m}^d=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\right)^\\mathbf{T}\\boldsymbol{R}_{\\Theta,n}^d$.\n",
    "\n",
    "由于 $\\boldsymbol{R}_{\\Theta,m}^d$ 的稀疏性，直接使用矩阵乘法会浪费算力，因此代码中采用下述方式实现：\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^{d}\\boldsymbol{x}=\\begin{pmatrix}x_{0}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\\\\\vdots\\\\x_{d-2}\\\\x_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_{0}\\\\\\cos m\\theta_{0}\\\\\\cos m\\theta_{1}\\\\\\cos m\\theta_{1}\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}+\\begin{pmatrix}-x_{1}\\\\x_{0}\\\\-x_{3}\\\\x_{2}\\\\\\vdots\\\\-x_{d-1}\\\\x_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_{0}\\\\\\sin m\\theta_{0}\\\\\\sin m\\theta_{1}\\\\\\sin m\\theta_{1}\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ecd80b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1271,  0.8812, -0.6388, -0.1893],\n",
       "          [ 1.0094, -1.1864, -0.0300,  0.7832],\n",
       "          [ 0.4957,  1.0637,  0.7216, -0.4009],\n",
       "          [-0.7514, -0.3974,  1.1031, -0.1514]],\n",
       "\n",
       "         [[ 1.4954, -0.1738,  0.0650, -2.2456],\n",
       "          [ 0.4194, -0.3086, -0.1484,  0.3030],\n",
       "          [ 1.0979,  1.3605,  0.9210, -0.6754],\n",
       "          [ 0.0564,  0.9247, -0.4771,  0.3487]],\n",
       "\n",
       "         [[ 0.4618, -0.9191, -0.4120,  0.6553],\n",
       "          [ 1.0198, -0.0600, -0.2367, -0.4534],\n",
       "          [-0.0824, -1.9700, -0.0934,  0.2286],\n",
       "          [-0.1301,  0.9609,  0.0657, -0.6252]],\n",
       "\n",
       "         [[ 1.3434, -0.8016,  1.6825,  0.1456],\n",
       "          [-0.0197, -2.1288,  1.7042,  0.0105],\n",
       "          [ 0.4340, -0.6796,  0.0223,  0.7759],\n",
       "          [-0.5726,  0.7877,  0.4440,  1.6066]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5729,  0.4159, -0.1599,  0.3882],\n",
       "          [-0.1739,  0.2431, -1.2901,  0.4096],\n",
       "          [-0.4292,  0.4393, -0.4322, -0.7097],\n",
       "          [ 0.1817,  1.2392,  0.2762,  0.7456]],\n",
       "\n",
       "         [[-0.0240,  0.1781,  0.1932,  0.8293],\n",
       "          [ 0.6627, -1.0248,  0.7747,  0.3207],\n",
       "          [-0.3718, -0.0591, -0.1886, -0.7853],\n",
       "          [-0.5692,  0.7138,  1.5670,  0.6982]],\n",
       "\n",
       "         [[ 0.7907,  0.6516, -0.1049,  0.6465],\n",
       "          [-1.1419, -0.1524,  1.8649,  0.6226],\n",
       "          [-0.1558, -1.5507, -1.0749,  0.0741],\n",
       "          [-0.0588, -1.3713,  0.1026,  1.1715]],\n",
       "\n",
       "         [[ 0.0072, -1.5438,  0.7856,  0.9387],\n",
       "          [-1.6276, -1.7842, -0.0946, -0.6614],\n",
       "          [ 1.2863, -1.6411, -0.1991, -0.2940],\n",
       "          [ 0.4994, -0.5400,  1.1822, -0.6059]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在 RoPE 中预先计算旋转角度对应的复数（cosθ + i·sinθ）值 mθ\n",
    "def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))  # \\theta_i = 10000^{-2i/d}, i \\in [0, 1, ..., d/2-1]\n",
    "    t = torch.arange(0, end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # [seq_len, dim]\n",
    "    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)\n",
    "    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    def rotate_half(x):\n",
    "        # 将 x 的前半部分和后半部分（取反）进行交换，代替 sin 的取反\n",
    "        return torch.cat([-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]], dim=-1)\n",
    "    \n",
    "    # q, k: [batch_size, seq_len, num_heads, head_dim]\n",
    "    # 对 q, k 和 cos, sin 进行广播运算，需要先匹配维度\n",
    "    # cos, sin [seq_len, head_dim] -> [(1), seq_len, 1, head_dim] 即对所有 batch, head 进行相同的广播运算\n",
    "    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))\n",
    "    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))\n",
    "    return q_embed, k_embed\n",
    "\n",
    "xq,  xk = torch.randn((2,  4,  4,  4)), torch.randn((2,  4,  4,  4)) # (batch_size,  sequence_length,  num_heads,  head_dim)\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(4,  4)\n",
    "q_embed, k_embed = apply_rotary_pos_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "q_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd577dd",
   "metadata": {},
   "source": [
    "### Attention\n",
    "注意力机制是 Transformer 架构的核心组件，能够有效捕捉长序列内各元素间的依赖关系，通过计算输入序列中不同位置元素间的注意力得分，对重要性进行建模\n",
    "\n",
    "在 MiniMindLM 模型中，Attention Block 包含下面的机制和模块：\n",
    "1. GQA (Group Query Attention) 分组查询注意力\n",
    "2. KV Cache\n",
    "3. SwiGLU\n",
    "#### GQA\n",
    "GQA 是对多头自注意力机制的扩展，通过对查询头分组，提高计算效率\n",
    "\n",
    "GQA 将 h 个查询头分为 G 组，每组包含 h / G 个查询头，共享一个公共的键和值\n",
    "\n",
    "**GQA 相比传统 MHA，减少了键和值的数量，降低了计算量和内存开销，提高了推理速度**\n",
    "\n",
    "### KV Cache\n",
    "在语言模型生成文本的过程中，每生成一个新的 token，模型都需要计算注意力得分，以确定当前位置与之前所有位置的相关性.\n",
    "\n",
    "比如以下内容：\n",
    "\n",
    "1. seq = [tok1] (位置 1):\n",
    "\n",
    "   S_1 = [ (Q1 * K1^T) / sqrt(d_k) ] (只有自己和自己的分数)\n",
    "\n",
    "   A_1 = softmax(S_1) = [1.0] (唯一选项，权重为1)\n",
    "\n",
    "   Output_1 = 1.0 * V1 = V1\n",
    "\n",
    "2. seq = [tok1, tok2] (计算位置 2 的输出):\n",
    "\n",
    "   S_2 = [ (Q2 * K1^T) / sqrt(d_k), (Q2 * K2^T) / sqrt(d_k) ] (位置2对位置1和位置2的分数)\n",
    "\n",
    "   A_2 = softmax(S_2) = [a_21, a_22] (对这两个分数进行归一化，a_21 + a_22 = 1)\n",
    "\n",
    "   Output_2 = a_21 * V1 + a_22 * V2\n",
    "\n",
    "3. seq = [tok1, tok2, tok3] (计算位置 3 的输出):\n",
    "\n",
    "   S_3 = [ (Q3 * K1^T) / sqrt(d_k), (Q3 * K2^T) / sqrt(d_k), (Q3 * K3^T) / sqrt(d_k) ] (位置3对位置1,2,3的分数)\n",
    "\n",
    "   A_3 = softmax(S_3) = [a_31, a_32, a_33] (对这三个分数进行归一化，a_31 + a_32 + a_33 = 1)\n",
    "   \n",
    "   Output_3 = a_31 * V1 + a_32 * V2 + a_33 * V3\n",
    "\n",
    "不难发现，大模型生成一个 token 后的注意力计算中，总会用到 token 序列的历史 KV 值，导致重复计算，KV Cache 的设计正是为了通过缓存历史 KV 值，节省计算开销.\n",
    "\n",
    "KV Cache 能够有效压缩大模型推理时的显存占用.\n",
    "\n",
    "注意力机制是在**计算某个位置的输出时，对该位置与所有可见位置（已生成的位置）的注意力分数进行一次性的 `softmax` 归一化**。\n",
    "\n",
    "#### 📌 总结与关键点\n",
    "\n",
    "1.  **`softmax` per Row (per Query)：** `softmax` 操作是针对**一个特定查询位置 `i`** 的所有（未被掩码的）键位置的分数进行归一化。它发生在计算该查询位置 `i` 的输出向量之前。\n",
    "2.  **注意力权重的意义：** 归一化后的注意力权重 `a_ij` 代表了 **位置 `i`（查询）对位置 `j`（键值）的“关注程度”**。所有权重之和为 1。\n",
    "3.  **输出是加权和：** 位置 `i` 的输出是其对所有可见位置 `j` 的值向量 `V_j` 的加权和，权重就是 `a_ij`。\n",
    "4.  **自回归生成中的缓存 (KV Cache)：** 在像GPT这样的Decoder-only模型进行自回归生成时（预测下一个token）：\n",
    "    *   当生成第 `i` 个 token 时，我们只需要计算**当前**的 `Q_i`。\n",
    "    *   所有之前位置 `j < i` 的 `K_j` 和 `V_j` 已经从之前的步骤中计算并**缓存**好了 (这就是著名的 **KV Cache**)。\n",
    "    *   因此，计算 `Output_i` 只需要：\n",
    "        *   计算 `Q_i`；\n",
    "        *   用 `Q_i` 和缓存的 `K_{1:i}` 计算分数 `S_i`；\n",
    "        *   对 `S_i` 做 `softmax` 得到 `A_i`；\n",
    "        *   用 `A_i` 和缓存的 `V_{1:i}` 计算加权和 `Output_i`。\n",
    "    *   计算完 `Output_i` 后，我们会计算并缓存**当前**位置的 `K_i` 和 `V_i`，供后续生成步骤使用。\n",
    "  \n",
    "### SwiGLU\n",
    "SwiGLU 是一种激活函数变体:\n",
    "$$\n",
    "SwiGLU(x, W, V, b, c) = Swish(xW+b) \\otimes (xV+c)\n",
    "$$\n",
    "其中 $Swish(x)=x \\cdot \\sigma (\\beta x)$\n",
    "\n",
    "与传统的 ReLU 激活函数相比，SwiGLU 具有更好的平滑性和非线性表达能力，由于其门控机制，在处理信息筛选和流动方面有独特的优势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7636a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class MiniMindConfig(PretrainedConfig):\n",
    "    model_type = \"minimind\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout: float = 0.0,\n",
    "        bos_token_id: int = 1,\n",
    "        eos_token_id: int = 2,\n",
    "        hidden_act: str = 'silu',\n",
    "        hidden_size: int = 512,\n",
    "        intermediate_size: int = None,\n",
    "        max_position_embeddings: int = 32768,\n",
    "        num_attention_heads: int = 8,\n",
    "        num_hidden_layers: int = 8,\n",
    "        num_key_value_heads: int = 2,\n",
    "        vocab_size: int = 6400,\n",
    "        rms_norm_eps: float = 1e-5,\n",
    "        rope_theta: int = 1000000,\n",
    "        flash_attn: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.hidden_act = hidden_act\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.flash_attn = flash_attn\n",
    "        \n",
    "        \n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"使 KV 头数适应 Query 头数， 执行矩阵乘法并行运算\n",
    "    等价于 torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    batch_size, seq_len, num_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]  # 等价于 x.unsqueeze(3)\n",
    "        .expand(batch_size, seq_len, num_kv_heads, n_rep, head_dim)\n",
    "        .reshape(batch_size, seq_len, num_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads\n",
    "        assert args.num_attention_heads % self.num_key_value_heads == 0\n",
    "        self.n_local_heads = args.num_attention_heads\n",
    "        self.n_local_kv_heads = args.num_key_value_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.hidden_size // args.num_attention_heads  # query 头映射的 head_dim\n",
    "        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(args.hidden_size, args.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(args.hidden_size, args.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        self.dropout = args.dropout\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn\n",
    "        \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # 接收 cos 和 sin\n",
    "                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "                use_cache=False,\n",
    "                attention_mask: Optional[torch.Tensor] = None,):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        ############## Forward QKV & RoPE ##############\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        \n",
    "        cos, sin = position_embeddings\n",
    "        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])  # 截断至 seq_len\n",
    "        \n",
    "        # kv_cache 实现\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0], xk], dim=1)  # 缓存每一个 token 的 k, v\n",
    "            xv = torch.cat([past_key_value[1], xk], dim=1)\n",
    "        past_kv = (xk, xv) if use_cache else None\n",
    "        \n",
    "        # [batch_size, seq_len, num_heads, head_dim] -> [bsz, num_heads, seq_len, head_dim]\n",
    "        xq, xk, xv = (\n",
    "            xq.transpose(1, 2),\n",
    "            repeat_kv(xk, self.n_rep).transpose(1, 2),\n",
    "            repeat_kv(xv, self.n_rep).transpose(1, 2),\n",
    "        )\n",
    "        \n",
    "        ############ Scaled Dot Production #############\n",
    "        if self.flash and seq_len != 1:\n",
    "            dropout_p = self.dropout if self.training else 0.0\n",
    "            attn_mask = None  # 这里的 attention_mask 指的是 padding 的掩码\n",
    "            if attention_mask is not None:\n",
    "                attn_mask = attention_mask.view(batch_size, 1, 1, -1).expand(batch_size, self.n_local_heads, seq_len, -1)  # attention_mask 形状为 [bsz, seq_len] 扩展后形状为 [bsz, n_heads, seq_len, seq_len]\n",
    "                attn_mask = attn_mask.bool()\n",
    "            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=True)\n",
    "        else:\n",
    "            # 普通注意力机制\n",
    "            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # 缩放点积\n",
    "            scores = scores + torch.triu(\n",
    "                torch.full((1, 1, seq_len, seq_len), float(\"inf\"), device=scores.device),\n",
    "                diagonal=1\n",
    "            )\n",
    "            \n",
    "            # 处理 padding 的掩码\n",
    "            if attention_mask is not None:\n",
    "                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9  # padding 的部分变为 -inf\n",
    "                scores += extended_attention_mask\n",
    "                \n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = scores @ xv  # [..., seq_len, seq_len] @ [..., seq_len, head_dim] -> [..., seq_len, head_dim]\n",
    "        \n",
    "        output = output.transpose(1, 2).reshape(batch_size, seq_len, -1)  # -> [batch_size, seq_len, dim] 等价于将所有头的输出维度拼接\n",
    "        output = self.resid_dropout(self.o_proj(output))\n",
    "        return output, past_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3081c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入张量 x ：size = torch.Size([2, 4, 512])，RoPE 旋转角： size = torch.Size([4, 64])\n",
      "输出 output: size = torch.Size([2, 4, 512]),  kv_cache 基本信息：size_key = torch.Size([2, 4, 2, 64]), size_value = torch.Size([2, 4, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "attn = Attention(MiniMindConfig())\n",
    "x = torch.randn((2, 4, 512))\n",
    "cos, sin = precompute_freqs_cis(64, 4)\n",
    "output, past_kv = attn(x, (cos, sin), use_cache=True)\n",
    "print(f'输入张量 x ：size = {x.shape}，RoPE 旋转角： size = {cos.shape}')\n",
    "print(f'输出 output: size = {output.shape},  kv_cache 基本信息：size_key = {past_kv[0].shape}, size_value = {past_kv[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "584cbe53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4768,  0.0344,  0.0520,  ..., -0.0625,  0.4398, -0.1725],\n",
       "         [-0.3904, -0.0107,  0.0797,  ..., -0.1729,  0.1587, -0.2356],\n",
       "         [-0.1888,  0.0010,  0.2464,  ..., -0.1216,  0.3145, -0.0996],\n",
       "         [-0.0146,  0.0089,  0.1481,  ..., -0.1093,  0.2050,  0.0232]],\n",
       "\n",
       "        [[-0.4153, -0.3545,  0.0203,  ...,  0.1221, -0.1954, -0.2919],\n",
       "         [-0.0517, -0.3404, -0.2075,  ...,  0.0648,  0.1257, -0.0926],\n",
       "         [ 0.1259, -0.2642, -0.1286,  ...,  0.1233, -0.0690, -0.1381],\n",
       "         [ 0.0247, -0.2238, -0.3681,  ...,  0.0936, -0.1268, -0.0488]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf789a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
