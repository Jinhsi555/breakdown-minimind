{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9bdfa5",
   "metadata": {},
   "source": [
    "# Model\n",
    "å½“ä»Šä¸»æµå¤§æ¨¡å‹ä»æ¶æ„ä¸Šå¤§è‡´å¯åˆ†ä¸ºç¨ å¯†ï¼ˆDenseï¼‰æ¨¡å‹å’Œæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture of Expert, MoEï¼‰æ¨¡å‹ã€‚\n",
    "\n",
    "ç¨ å¯†æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°åœ¨æ¯æ¬¡è®¡ç®—æ—¶éƒ½ä¼šå‚ä¸è¿ç®—ï¼›æ··åˆä¸“å®¶æ¨¡å‹åˆ™å°†ä¸åŒçš„â€œä¸“å®¶â€æ¨¡å—ç»„åˆï¼Œæ ¹æ®è¾“å…¥é€‰æ‹©åˆé€‚çš„ä¸“å®¶å¤„ç†ï¼Œèƒ½åœ¨ä¿è¯æ•ˆæœçš„åŒæ—¶å‡å°‘è®¡ç®—é‡å’Œå‚æ•°é‡ã€‚\n",
    "\n",
    "MiniMind æ¨¡å‹åœ¨Llama 3.1 çš„åŸºç¡€ä¸Šè®¾è®¡ï¼ŒåŸºäºç»å…¸çš„Transformer Decoder-Only æ¶æ„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5692f5f",
   "metadata": {},
   "source": [
    "## MiniMind Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ffaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import struct\n",
    "import inspect\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from model.LMConfig import LMConfig\n",
    "from typing import Any, Optional, Tuple, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f47e3",
   "metadata": {},
   "source": [
    "### å‡æ–¹æ ¹å±‚å½’ä¸€åŒ–ï¼ˆRMSNormï¼‰\n",
    "RMSNorm æ˜¯å¯¹ LayerNorm çš„æ”¹è¿›ï¼Œç§»é™¤äº†å‡å€¼é¡¹ï¼Œå¯ä»¥è§†ä¸º LayerNormåœ¨å‡å€¼ä¸º 0 æ—¶çš„ç‰¹ä¾‹ã€‚\n",
    "* LayerNorm\n",
    "$$\n",
    "y = \\frac{x- E(x)} {\\sqrt {Var(x)+\\epsilon}} * \\gamma + \\beta\n",
    "$$\n",
    "* RMS Norm\n",
    "$$\n",
    "a_i = \\frac {a_i}{RMS(a)+\\epsilon}*\\gamma, \\ where \\ RMS(a) = \\sqrt { \\frac {1} {n} \\sum_{i=1}^{n} a^2_i}\n",
    "$$\n",
    "RMS Normåœ¨Layer Normçš„åŸºç¡€ä¸Šèˆå¼ƒäº†ä¸­å¿ƒåŒ–æ“ä½œï¼Œä»…ç”¨ç¼©æ”¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œå…¶ä¸æ”¹å˜æ•°æ®åŸæœ¬çš„åˆ†å¸ƒï¼Œæœ‰åˆ©äºæ¿€æ´»å‡½æ•°è¾“å‡ºçš„ç¨³å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac41d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # x: [batch_size, seq_len, dim]\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)  # è¾“å‡ºä¸è¾“å…¥çš„æ•°æ®ç±»å‹ä¸€è‡´ï¼Œé¿å…åç»­è®¡ç®—ç ´åæ··åˆç²¾åº¦è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8d57c",
   "metadata": {},
   "source": [
    "## Rotary Position Embedding(RoPE) æ—‹è½¬ä½ç½®ç¼–ç \n",
    "### Rotary Position Embedding, RoPE\n",
    "\n",
    "æ—‹è½¬ä½ç½®ç¼–ç æ˜¯ä¸€ç§èƒ½å°†ç›¸å¯¹ä½ç½®ä¿¡æ¯é›†æˆåˆ° self-attention ä¸­, è¿›è€Œæå‡ transformer æ¶æ„æ€§èƒ½çš„ä½ç½®ç¼–ç æ–¹å¼, å’Œç»å¯¹ä½ç½®ç¼–ç ç›¸æ¯”, RoPE å…·æœ‰å¾ˆå¥½çš„å¤–æ¨æ€§, æ˜¯ç›®å‰çš„ä¸»æµä½ç½®ç¼–ç æ–¹å¼.\n",
    "\n",
    "å¤–æ¨æ€§çš„è§£é‡Š, é€šä¿—æ¥è¯´å°±æ˜¯è®­ç»ƒçš„æ—¶å€™é™åˆ¶äº† 512 çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé‚£ä¹ˆæ¨ç†æ—¶å¦‚æœé¢å¯¹è¶…è¿‡è¯¥é•¿åº¦çš„æ–‡æœ¬ï¼ŒLLM å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†.\n",
    "\n",
    "- **ç»å¯¹ä½ç½®ç¼–ç **\n",
    "\n",
    "ç»å¯¹ä½ç½®ç¼–ç æ˜¯æ—©æœŸ Transformer æ¶æ„é‡‡ç”¨çš„ç»å¯¹ä½ç½®ç¼–ç æ–¹æ¡ˆï¼ŒåŠé‚£ä¸ªæ¯ä¸ªä½ç½®æ˜ å°„ä¸ºå›ºå®šçš„å‘é‡è¡¨ç¤º.\n",
    "\n",
    "$$f_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i,i)=\\boldsymbol{W}_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i+\\boldsymbol{p}_i)$$\n",
    "\n",
    "å…¶ä¸­ç¼–ç å‘é‡ $p_i$ çš„è®¡ç®—ä½¿ç”¨å¦‚ä¸‹å…¬å¼ï¼š\n",
    "\n",
    "$$\\boldsymbol{p}_{i,2t}=\\sin\\left(k/1000^{2t/d}\\right), \\boldsymbol{p}_{i,2t+1}=\\cos\\left(k/1000^{2t/d}\\right)$$\n",
    "\n",
    "æ­£å¦‚å…¶åï¼Œç»å¯¹ä½ç½®ç¼–ç åªè€ƒè™‘äº†è¾“å…¥åºåˆ—ä¸­çš„ç»å¯¹ä½ç½®å…³ç³»ï¼Œå¯¹äº token ä¹‹é—´çš„ç›¸å¯¹ä¿¡æ¯åˆ™æ²¡æœ‰çº³å…¥è€ƒè™‘.\n",
    "\n",
    "- **æ—‹è½¬ä½ç½®ç¼–ç **\n",
    "\n",
    "å‡å®š query å’Œ key çš„å†…ç§¯æ“ä½œå¯ä»¥è¢«å‡½æ•° g è¡¨ç¤ºï¼Œè¯¥å‡½æ•° g çš„è¾“å…¥æ˜¯è¯åµŒå…¥å‘é‡ $x_m, x_n$ å’Œå®ƒä»¬ä¹‹é—´çš„ç›¸å¯¹ä½ç½® $m-n$:\n",
    "\n",
    "$$<f_q(x_m ,m), f_k(x_n, n)>=g(x_m, x_n, m, n)$$\n",
    "\n",
    "æ—‹è½¬ä½ç½®ç¼–ç å°±æ˜¯æ‰¾åˆ°ä¸€ä¸ªä½¿ä¸Šå¼æˆç«‹çš„ä½ç½®ç¼–ç æ–¹å¼. \n",
    "\n",
    "å‡ºäºè®¤è¯†çš„ç›®çš„ï¼Œæˆ‘ä»¬çœç•¥å¤æ‚çš„æ•°å­¦æ¨å¯¼ï¼Œç›´æ¥çœ‹ RoPE çš„çš„ç»“è®ºï¼š\n",
    "\n",
    "å­˜åœ¨è¿™æ ·ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼š\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^d=\\underbrace{\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\end{pmatrix}}_{\\boldsymbol{W}_m}$$\n",
    "\n",
    "å…¶ä¸­ï¼Œ$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å°† query å’Œ key çš„å†…ç§¯æ“ä½œè½¬æ¢ä¸ºä¸åŸå§‹å‘é‡ $x$ ç›¸å…³çš„ä»¥ä¸‹ç­‰ä»·å½¢å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\boldsymbol{q}_m^\\mathbf{T}\\boldsymbol{k}_n=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)^\\mathbf{T}\\left(\\boldsymbol{R}_{\\Theta,n}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\\right)=\\boldsymbol{x}_m^\\mathbf{T}\\boldsymbol{W}_q\\boldsymbol{R}_{\\Theta,n-m}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼Œ $\\boldsymbol{R}_{\\Theta,n-m}^d=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\right)^\\mathbf{T}\\boldsymbol{R}_{\\Theta,n}^d$.\n",
    "\n",
    "ç”±äº $\\boldsymbol{R}_{\\Theta,m}^d$ çš„ç¨€ç–æ€§ï¼Œç›´æ¥ä½¿ç”¨çŸ©é˜µä¹˜æ³•ä¼šæµªè´¹ç®—åŠ›ï¼Œå› æ­¤ä»£ç ä¸­é‡‡ç”¨ä¸‹è¿°æ–¹å¼å®ç°ï¼š\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^{d}\\boldsymbol{x}=\\begin{pmatrix}x_{0}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\\\\\vdots\\\\x_{d-2}\\\\x_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_{0}\\\\\\cos m\\theta_{0}\\\\\\cos m\\theta_{1}\\\\\\cos m\\theta_{1}\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}+\\begin{pmatrix}-x_{1}\\\\x_{0}\\\\-x_{3}\\\\x_{2}\\\\\\vdots\\\\-x_{d-1}\\\\x_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_{0}\\\\\\sin m\\theta_{0}\\\\\\sin m\\theta_{1}\\\\\\sin m\\theta_{1}\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ecd80b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1271,  0.8812, -0.6388, -0.1893],\n",
       "          [ 1.0094, -1.1864, -0.0300,  0.7832],\n",
       "          [ 0.4957,  1.0637,  0.7216, -0.4009],\n",
       "          [-0.7514, -0.3974,  1.1031, -0.1514]],\n",
       "\n",
       "         [[ 1.4954, -0.1738,  0.0650, -2.2456],\n",
       "          [ 0.4194, -0.3086, -0.1484,  0.3030],\n",
       "          [ 1.0979,  1.3605,  0.9210, -0.6754],\n",
       "          [ 0.0564,  0.9247, -0.4771,  0.3487]],\n",
       "\n",
       "         [[ 0.4618, -0.9191, -0.4120,  0.6553],\n",
       "          [ 1.0198, -0.0600, -0.2367, -0.4534],\n",
       "          [-0.0824, -1.9700, -0.0934,  0.2286],\n",
       "          [-0.1301,  0.9609,  0.0657, -0.6252]],\n",
       "\n",
       "         [[ 1.3434, -0.8016,  1.6825,  0.1456],\n",
       "          [-0.0197, -2.1288,  1.7042,  0.0105],\n",
       "          [ 0.4340, -0.6796,  0.0223,  0.7759],\n",
       "          [-0.5726,  0.7877,  0.4440,  1.6066]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5729,  0.4159, -0.1599,  0.3882],\n",
       "          [-0.1739,  0.2431, -1.2901,  0.4096],\n",
       "          [-0.4292,  0.4393, -0.4322, -0.7097],\n",
       "          [ 0.1817,  1.2392,  0.2762,  0.7456]],\n",
       "\n",
       "         [[-0.0240,  0.1781,  0.1932,  0.8293],\n",
       "          [ 0.6627, -1.0248,  0.7747,  0.3207],\n",
       "          [-0.3718, -0.0591, -0.1886, -0.7853],\n",
       "          [-0.5692,  0.7138,  1.5670,  0.6982]],\n",
       "\n",
       "         [[ 0.7907,  0.6516, -0.1049,  0.6465],\n",
       "          [-1.1419, -0.1524,  1.8649,  0.6226],\n",
       "          [-0.1558, -1.5507, -1.0749,  0.0741],\n",
       "          [-0.0588, -1.3713,  0.1026,  1.1715]],\n",
       "\n",
       "         [[ 0.0072, -1.5438,  0.7856,  0.9387],\n",
       "          [-1.6276, -1.7842, -0.0946, -0.6614],\n",
       "          [ 1.2863, -1.6411, -0.1991, -0.2940],\n",
       "          [ 0.4994, -0.5400,  1.1822, -0.6059]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åœ¨ RoPE ä¸­é¢„å…ˆè®¡ç®—æ—‹è½¬è§’åº¦å¯¹åº”çš„å¤æ•°ï¼ˆcosÎ¸ + iÂ·sinÎ¸ï¼‰å€¼ mÎ¸\n",
    "def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))  # \\theta_i = 10000^{-2i/d}, i \\in [0, 1, ..., d/2-1]\n",
    "    t = torch.arange(0, end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # [seq_len, dim]\n",
    "    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)\n",
    "    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    def rotate_half(x):\n",
    "        # å°† x çš„å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†ï¼ˆå–åï¼‰è¿›è¡Œäº¤æ¢ï¼Œä»£æ›¿ sin çš„å–å\n",
    "        return torch.cat([-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]], dim=-1)\n",
    "    \n",
    "    # q, k: [batch_size, seq_len, num_heads, head_dim]\n",
    "    # å¯¹ q, k å’Œ cos, sin è¿›è¡Œå¹¿æ’­è¿ç®—ï¼Œéœ€è¦å…ˆåŒ¹é…ç»´åº¦\n",
    "    # cos, sin [seq_len, head_dim] -> [(1), seq_len, 1, head_dim] å³å¯¹æ‰€æœ‰ batch, head è¿›è¡Œç›¸åŒçš„å¹¿æ’­è¿ç®—\n",
    "    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))\n",
    "    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))\n",
    "    return q_embed, k_embed\n",
    "\n",
    "xq,  xk = torch.randn((2,  4,  4,  4)), torch.randn((2,  4,  4,  4)) # (batch_size,  sequence_length,  num_heads,  head_dim)\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(4,  4)\n",
    "q_embed, k_embed = apply_rotary_pos_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "q_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd577dd",
   "metadata": {},
   "source": [
    "### Attention\n",
    "æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer æ¶æ„çš„æ ¸å¿ƒç»„ä»¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é•¿åºåˆ—å†…å„å…ƒç´ é—´çš„ä¾èµ–å…³ç³»ï¼Œé€šè¿‡è®¡ç®—è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®å…ƒç´ é—´çš„æ³¨æ„åŠ›å¾—åˆ†ï¼Œå¯¹é‡è¦æ€§è¿›è¡Œå»ºæ¨¡\n",
    "\n",
    "åœ¨ MiniMindLM æ¨¡å‹ä¸­ï¼ŒAttention Block åŒ…å«ä¸‹é¢çš„æœºåˆ¶å’Œæ¨¡å—ï¼š\n",
    "1. GQA (Group Query Attention) åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›\n",
    "2. KV Cache\n",
    "3. SwiGLU\n",
    "#### GQA\n",
    "GQA æ˜¯å¯¹å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ‰©å±•ï¼Œé€šè¿‡å¯¹æŸ¥è¯¢å¤´åˆ†ç»„ï¼Œæé«˜è®¡ç®—æ•ˆç‡\n",
    "\n",
    "GQA å°† h ä¸ªæŸ¥è¯¢å¤´åˆ†ä¸º G ç»„ï¼Œæ¯ç»„åŒ…å« h / G ä¸ªæŸ¥è¯¢å¤´ï¼Œå…±äº«ä¸€ä¸ªå…¬å…±çš„é”®å’Œå€¼\n",
    "\n",
    "**GQA ç›¸æ¯”ä¼ ç»Ÿ MHAï¼Œå‡å°‘äº†é”®å’Œå€¼çš„æ•°é‡ï¼Œé™ä½äº†è®¡ç®—é‡å’Œå†…å­˜å¼€é”€ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦**\n",
    "\n",
    "### KV Cache\n",
    "åœ¨è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ä¸­ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–°çš„ tokenï¼Œæ¨¡å‹éƒ½éœ€è¦è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œä»¥ç¡®å®šå½“å‰ä½ç½®ä¸ä¹‹å‰æ‰€æœ‰ä½ç½®çš„ç›¸å…³æ€§.\n",
    "\n",
    "æ¯”å¦‚ä»¥ä¸‹å†…å®¹ï¼š\n",
    "\n",
    "1. seq = [tok1] (ä½ç½® 1):\n",
    "\n",
    "   S_1 = [ (Q1 * K1^T) / sqrt(d_k) ] (åªæœ‰è‡ªå·±å’Œè‡ªå·±çš„åˆ†æ•°)\n",
    "\n",
    "   A_1 = softmax(S_1) = [1.0] (å”¯ä¸€é€‰é¡¹ï¼Œæƒé‡ä¸º1)\n",
    "\n",
    "   Output_1 = 1.0 * V1 = V1\n",
    "\n",
    "2. seq = [tok1, tok2] (è®¡ç®—ä½ç½® 2 çš„è¾“å‡º):\n",
    "\n",
    "   S_2 = [ (Q2 * K1^T) / sqrt(d_k), (Q2 * K2^T) / sqrt(d_k) ] (ä½ç½®2å¯¹ä½ç½®1å’Œä½ç½®2çš„åˆ†æ•°)\n",
    "\n",
    "   A_2 = softmax(S_2) = [a_21, a_22] (å¯¹è¿™ä¸¤ä¸ªåˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œa_21 + a_22 = 1)\n",
    "\n",
    "   Output_2 = a_21 * V1 + a_22 * V2\n",
    "\n",
    "3. seq = [tok1, tok2, tok3] (è®¡ç®—ä½ç½® 3 çš„è¾“å‡º):\n",
    "\n",
    "   S_3 = [ (Q3 * K1^T) / sqrt(d_k), (Q3 * K2^T) / sqrt(d_k), (Q3 * K3^T) / sqrt(d_k) ] (ä½ç½®3å¯¹ä½ç½®1,2,3çš„åˆ†æ•°)\n",
    "\n",
    "   A_3 = softmax(S_3) = [a_31, a_32, a_33] (å¯¹è¿™ä¸‰ä¸ªåˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œa_31 + a_32 + a_33 = 1)\n",
    "   \n",
    "   Output_3 = a_31 * V1 + a_32 * V2 + a_33 * V3\n",
    "\n",
    "ä¸éš¾å‘ç°ï¼Œå¤§æ¨¡å‹ç”Ÿæˆä¸€ä¸ª token åçš„æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œæ€»ä¼šç”¨åˆ° token åºåˆ—çš„å†å² KV å€¼ï¼Œå¯¼è‡´é‡å¤è®¡ç®—ï¼ŒKV Cache çš„è®¾è®¡æ­£æ˜¯ä¸ºäº†é€šè¿‡ç¼“å­˜å†å² KV å€¼ï¼ŒèŠ‚çœè®¡ç®—å¼€é”€.\n",
    "\n",
    "KV Cache èƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©å¤§æ¨¡å‹æ¨ç†æ—¶çš„æ˜¾å­˜å ç”¨.\n",
    "\n",
    "æ³¨æ„åŠ›æœºåˆ¶æ˜¯åœ¨**è®¡ç®—æŸä¸ªä½ç½®çš„è¾“å‡ºæ—¶ï¼Œå¯¹è¯¥ä½ç½®ä¸æ‰€æœ‰å¯è§ä½ç½®ï¼ˆå·²ç”Ÿæˆçš„ä½ç½®ï¼‰çš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œä¸€æ¬¡æ€§çš„ `softmax` å½’ä¸€åŒ–**ã€‚\n",
    "\n",
    "#### ğŸ“Œ æ€»ç»“ä¸å…³é”®ç‚¹\n",
    "\n",
    "1.  **`softmax` per Row (per Query)ï¼š** `softmax` æ“ä½œæ˜¯é’ˆå¯¹**ä¸€ä¸ªç‰¹å®šæŸ¥è¯¢ä½ç½® `i`** çš„æ‰€æœ‰ï¼ˆæœªè¢«æ©ç çš„ï¼‰é”®ä½ç½®çš„åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ã€‚å®ƒå‘ç”Ÿåœ¨è®¡ç®—è¯¥æŸ¥è¯¢ä½ç½® `i` çš„è¾“å‡ºå‘é‡ä¹‹å‰ã€‚\n",
    "2.  **æ³¨æ„åŠ›æƒé‡çš„æ„ä¹‰ï¼š** å½’ä¸€åŒ–åçš„æ³¨æ„åŠ›æƒé‡ `a_ij` ä»£è¡¨äº† **ä½ç½® `i`ï¼ˆæŸ¥è¯¢ï¼‰å¯¹ä½ç½® `j`ï¼ˆé”®å€¼ï¼‰çš„â€œå…³æ³¨ç¨‹åº¦â€**ã€‚æ‰€æœ‰æƒé‡ä¹‹å’Œä¸º 1ã€‚\n",
    "3.  **è¾“å‡ºæ˜¯åŠ æƒå’Œï¼š** ä½ç½® `i` çš„è¾“å‡ºæ˜¯å…¶å¯¹æ‰€æœ‰å¯è§ä½ç½® `j` çš„å€¼å‘é‡ `V_j` çš„åŠ æƒå’Œï¼Œæƒé‡å°±æ˜¯ `a_ij`ã€‚\n",
    "4.  **è‡ªå›å½’ç”Ÿæˆä¸­çš„ç¼“å­˜ (KV Cache)ï¼š** åœ¨åƒGPTè¿™æ ·çš„Decoder-onlyæ¨¡å‹è¿›è¡Œè‡ªå›å½’ç”Ÿæˆæ—¶ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰ï¼š\n",
    "    *   å½“ç”Ÿæˆç¬¬ `i` ä¸ª token æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—**å½“å‰**çš„ `Q_i`ã€‚\n",
    "    *   æ‰€æœ‰ä¹‹å‰ä½ç½® `j < i` çš„ `K_j` å’Œ `V_j` å·²ç»ä»ä¹‹å‰çš„æ­¥éª¤ä¸­è®¡ç®—å¹¶**ç¼“å­˜**å¥½äº† (è¿™å°±æ˜¯è‘—åçš„ **KV Cache**)ã€‚\n",
    "    *   å› æ­¤ï¼Œè®¡ç®— `Output_i` åªéœ€è¦ï¼š\n",
    "        *   è®¡ç®— `Q_i`ï¼›\n",
    "        *   ç”¨ `Q_i` å’Œç¼“å­˜çš„ `K_{1:i}` è®¡ç®—åˆ†æ•° `S_i`ï¼›\n",
    "        *   å¯¹ `S_i` åš `softmax` å¾—åˆ° `A_i`ï¼›\n",
    "        *   ç”¨ `A_i` å’Œç¼“å­˜çš„ `V_{1:i}` è®¡ç®—åŠ æƒå’Œ `Output_i`ã€‚\n",
    "    *   è®¡ç®—å®Œ `Output_i` åï¼Œæˆ‘ä»¬ä¼šè®¡ç®—å¹¶ç¼“å­˜**å½“å‰**ä½ç½®çš„ `K_i` å’Œ `V_i`ï¼Œä¾›åç»­ç”Ÿæˆæ­¥éª¤ä½¿ç”¨ã€‚\n",
    "  \n",
    "### SwiGLU\n",
    "SwiGLU æ˜¯ä¸€ç§æ¿€æ´»å‡½æ•°å˜ä½“:\n",
    "$$\n",
    "SwiGLU(x, W, V, b, c) = Swish(xW+b) \\otimes (xV+c)\n",
    "$$\n",
    "å…¶ä¸­ $Swish(x)=x \\cdot \\sigma (\\beta x)$\n",
    "\n",
    "ä¸ä¼ ç»Ÿçš„ ReLU æ¿€æ´»å‡½æ•°ç›¸æ¯”ï¼ŒSwiGLU å…·æœ‰æ›´å¥½çš„å¹³æ»‘æ€§å’Œéçº¿æ€§è¡¨è¾¾èƒ½åŠ›ï¼Œç”±äºå…¶é—¨æ§æœºåˆ¶ï¼Œåœ¨å¤„ç†ä¿¡æ¯ç­›é€‰å’ŒæµåŠ¨æ–¹é¢æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7636a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class MiniMindConfig(PretrainedConfig):\n",
    "    model_type = \"minimind\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout: float = 0.0,\n",
    "        bos_token_id: int = 1,\n",
    "        eos_token_id: int = 2,\n",
    "        hidden_act: str = 'silu',\n",
    "        hidden_size: int = 512,\n",
    "        intermediate_size: int = None,\n",
    "        max_position_embeddings: int = 32768,\n",
    "        num_attention_heads: int = 8,\n",
    "        num_hidden_layers: int = 8,\n",
    "        num_key_value_heads: int = 2,\n",
    "        vocab_size: int = 6400,\n",
    "        rms_norm_eps: float = 1e-5,\n",
    "        rope_theta: int = 1000000,\n",
    "        flash_attn: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.hidden_act = hidden_act\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.flash_attn = flash_attn\n",
    "        \n",
    "        \n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"ä½¿ KV å¤´æ•°é€‚åº” Query å¤´æ•°ï¼Œ æ‰§è¡ŒçŸ©é˜µä¹˜æ³•å¹¶è¡Œè¿ç®—\n",
    "    ç­‰ä»·äº torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    batch_size, seq_len, num_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]  # ç­‰ä»·äº x.unsqueeze(3)\n",
    "        .expand(batch_size, seq_len, num_kv_heads, n_rep, head_dim)\n",
    "        .reshape(batch_size, seq_len, num_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads\n",
    "        assert args.num_attention_heads % self.num_key_value_heads == 0\n",
    "        self.n_local_heads = args.num_attention_heads\n",
    "        self.n_local_kv_heads = args.num_key_value_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.hidden_size // args.num_attention_heads  # query å¤´æ˜ å°„çš„ head_dim\n",
    "        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(args.hidden_size, args.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(args.hidden_size, args.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        self.dropout = args.dropout\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn\n",
    "        \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # æ¥æ”¶ cos å’Œ sin\n",
    "                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "                use_cache=False,\n",
    "                attention_mask: Optional[torch.Tensor] = None,):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        ############## Forward QKV & RoPE ##############\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        \n",
    "        cos, sin = position_embeddings\n",
    "        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])  # æˆªæ–­è‡³ seq_len\n",
    "        \n",
    "        # kv_cache å®ç°\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0], xk], dim=1)  # ç¼“å­˜æ¯ä¸€ä¸ª token çš„ k, v\n",
    "            xv = torch.cat([past_key_value[1], xk], dim=1)\n",
    "        past_kv = (xk, xv) if use_cache else None\n",
    "        \n",
    "        # [batch_size, seq_len, num_heads, head_dim] -> [bsz, num_heads, seq_len, head_dim]\n",
    "        xq, xk, xv = (\n",
    "            xq.transpose(1, 2),\n",
    "            repeat_kv(xk, self.n_rep).transpose(1, 2),\n",
    "            repeat_kv(xv, self.n_rep).transpose(1, 2),\n",
    "        )\n",
    "        \n",
    "        ############ Scaled Dot Production #############\n",
    "        if self.flash and seq_len != 1:\n",
    "            dropout_p = self.dropout if self.training else 0.0\n",
    "            attn_mask = None  # è¿™é‡Œçš„ attention_mask æŒ‡çš„æ˜¯ padding çš„æ©ç \n",
    "            if attention_mask is not None:\n",
    "                attn_mask = attention_mask.view(batch_size, 1, 1, -1).expand(batch_size, self.n_local_heads, seq_len, -1)  # attention_mask å½¢çŠ¶ä¸º [bsz, seq_len] æ‰©å±•åå½¢çŠ¶ä¸º [bsz, n_heads, seq_len, seq_len]\n",
    "                attn_mask = attn_mask.bool()\n",
    "            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=True)\n",
    "        else:\n",
    "            # æ™®é€šæ³¨æ„åŠ›æœºåˆ¶\n",
    "            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # ç¼©æ”¾ç‚¹ç§¯\n",
    "            scores = scores + torch.triu(\n",
    "                torch.full((1, 1, seq_len, seq_len), float(\"inf\"), device=scores.device),\n",
    "                diagonal=1\n",
    "            )\n",
    "            \n",
    "            # å¤„ç† padding çš„æ©ç \n",
    "            if attention_mask is not None:\n",
    "                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9  # padding çš„éƒ¨åˆ†å˜ä¸º -inf\n",
    "                scores += extended_attention_mask\n",
    "                \n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = scores @ xv  # [..., seq_len, seq_len] @ [..., seq_len, head_dim] -> [..., seq_len, head_dim]\n",
    "        \n",
    "        output = output.transpose(1, 2).reshape(batch_size, seq_len, -1)  # -> [batch_size, seq_len, dim] ç­‰ä»·äºå°†æ‰€æœ‰å¤´çš„è¾“å‡ºç»´åº¦æ‹¼æ¥\n",
    "        output = self.resid_dropout(self.o_proj(output))\n",
    "        return output, past_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3081c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å¼ é‡ x ï¼šsize = torch.Size([2, 4, 512])ï¼ŒRoPE æ—‹è½¬è§’ï¼š size = torch.Size([4, 64])\n",
      "è¾“å‡º output: size = torch.Size([2, 4, 512]),  kv_cache åŸºæœ¬ä¿¡æ¯ï¼šsize_key = torch.Size([2, 4, 2, 64]), size_value = torch.Size([2, 4, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "attn = Attention(MiniMindConfig())\n",
    "x = torch.randn((2, 4, 512))\n",
    "cos, sin = precompute_freqs_cis(64, 4)\n",
    "output, past_kv = attn(x, (cos, sin), use_cache=True)\n",
    "print(f'è¾“å…¥å¼ é‡ x ï¼šsize = {x.shape}ï¼ŒRoPE æ—‹è½¬è§’ï¼š size = {cos.shape}')\n",
    "print(f'è¾“å‡º output: size = {output.shape},  kv_cache åŸºæœ¬ä¿¡æ¯ï¼šsize_key = {past_kv[0].shape}, size_value = {past_kv[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "584cbe53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4768,  0.0344,  0.0520,  ..., -0.0625,  0.4398, -0.1725],\n",
       "         [-0.3904, -0.0107,  0.0797,  ..., -0.1729,  0.1587, -0.2356],\n",
       "         [-0.1888,  0.0010,  0.2464,  ..., -0.1216,  0.3145, -0.0996],\n",
       "         [-0.0146,  0.0089,  0.1481,  ..., -0.1093,  0.2050,  0.0232]],\n",
       "\n",
       "        [[-0.4153, -0.3545,  0.0203,  ...,  0.1221, -0.1954, -0.2919],\n",
       "         [-0.0517, -0.3404, -0.2075,  ...,  0.0648,  0.1257, -0.0926],\n",
       "         [ 0.1259, -0.2642, -0.1286,  ...,  0.1233, -0.0690, -0.1381],\n",
       "         [ 0.0247, -0.2238, -0.3681,  ...,  0.0936, -0.1268, -0.0488]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf789a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
